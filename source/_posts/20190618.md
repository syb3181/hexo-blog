---
title: 20190618
date: 2019-06-18 11:01:24
tags: ml
---

很多霸格。

## 1.
前一阵去行内测试模型的效果，然后发现预估的时候，每次结果都不一样。当时场面很尴尬。
回去以后想起来有可能是dropout的事。nn.Dropout点进去看是F.dropout，
有个参数是self.training, 如果是训练就抹掉一些边，如果是预估，就不抹。
所以在预估之前设置为预估模式就行了。
{% codeblock lang:python %}
model.eval()
{% endcodeblock %}

后来重新实现BiDAF，一开始发现怎么都不对，好几个epoch以后EM(ExactMatch)还是小于0.1。

## 2.
没有mask。进RNN之前，算Attention，都要加上mask。
{% codeblock lang:python %}
class MaskedLSTMEncoder(nn.Module):

    def __init__(self, input_size, hidden_size, bidirectional, batch_first, num_layers, dropout):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            batch_first=batch_first,
            bidirectional=bidirectional,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0
        )
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, x_lens):
        lstm_input = pack_padded_sequence(x, x_lens, enforce_sorted=False, batch_first=True)
        m, _ = self.lstm(lstm_input)
        m, _ = pad_packed_sequence(m, batch_first=True)
        m = self.dropout(m)
        return m
{% endcodeblock %}
enforce_sorted是因为batch的时候，一个是context，一个是query，
他们的长度不能同时排序。
所以不能保证一个batch里面，数据的长度是递减的。所以要排序。
注意这个开关需要torch版本1.1.0才可以开启。如果不想升版本也没问题，手动记一下顺序。
参考这个[starter code](https://github.com/chrischute/squad])。


## 3.
另一个霸格是cross_entropy的code没有好好读，原来传进去的logits不是一个分布。
cross_entroy = softmax + nll_loss。logits本身没啥问题，就是pad的部分，
一开始我用的0，这样如果用cross_entropy的话，pad的地方就变成了1，然后再加权平均的话，
pad的地方就被算上了。有个方法，就是先去过拟合一些小数据，如果有这种错误的话，
很可能小数据都不能过拟合，loss可能也爆炸。

## 4.
还有一个大霸格，说出来我都不太好意思，就是
{% codeblock lang:python %}
self.word_embedding = nn.Embedding(..)
nn.Embedding.from_pretrained(torch.from_numpy(w).cuda(), padding_idx=1)
{% endcodeblock %}
后来我发现我BiDAF的loss到3.9就不动了，然后上面的starter code能随便到3.3。
然后我把自己的Embedding去掉了，几乎没变化
（其实之前我还把starter code的数据处理和一部分代码替换进来，loss降了，确定了错误的范围）。

甚是蛋疼。浪费了好多时间debug。
