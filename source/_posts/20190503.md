---
title: 20190503
date: 2019-05-03 14:19:53
tags:
---

随便记录一点吧在做某银行法律文书信息抽取项目的事情。

## 1. 环境

### 1. 保密
银行是真的保密。由于保密的需要，所有的内容都是只进不出。
比如标注数据，我们项目用的标注数据一共就五百篇左右的法律文书。
这些法律文书都可以从判决文书网下载，即便如此，银行也不允许把数据从行内拿出来。

### 2. U盘
所有的服务器都是处于断外网状态的，连系统的依赖都需要在服务器下载好了然后。然后通过行内提供的Windows笔记本，用U盘复制到服务器上。U盘的文件系统格式我推荐FAT32，买了个金士顿的带type-C接口的U盘，这样Linux台式机，MacOS的笔记本，Windows都能读写。唯一的问题就是镜像如果超过4G就很麻烦，这时候需要准备一个NTFS格式的U盘。

### 3. 基础镜像
应用要部署到银行的机器学习平台需要用银行给的基础镜像。这个基础镜像locale设置有问题，打印中文会报错。只能从行外下载好依赖，写进Dockerfile，通过这样的方式来修复（当然不打印中文也是可以的）。


## 2. 模型

抽取实际上搞成了一个NER的问题。把序列标上BIO。
比如
{% codeblock %}
多(B-人名)田(I)从(O)方(O)向(O)盘(O)抽(O)出(O)一(O)只(O)手(O)。(O)
{% endcodeblock %}

一开始博士小哥整了个bert + CRF，似乎有内存泄露的问题，每次都要手动杀进程。除此之外，性能还是很好的，在行外从训练集分出来的验证集上就87的f1了，行内应该更高，而且在随手抓的文书上效果也很不错，真是挺神奇的，毕竟训练的数据实在是太少了。

但是合作单位那边的技术负责人不太满意，因为有4000W+文书需要处理，行内只给了一片P40，要跑40天。还不是独占的。bert用的是官方的，tensorflowhub有两种规格的，从A24到A12，然后maxlen从200到50，这样batchsize也从16变成了64，这样处理速度能达到40sent/s。算下来还要20天，加上APEC（似乎是降低精度提高速度，我让一个小哥试了，快了一点）加速。于是我们开始换模型。

于是我就去试了BiLSTM，DGCNN，比BERT差了5个点，也可能是我模型写的有问题。我参照了[cs230的一份code](https://github.com/cs230-stanford/cs230-code-examples/tree/master/pytorch/nlp)来组织项目。pytorch用来做实验不错，可能最后上生产还是要回归keras和tf。上线似乎行内的标准是要用tensorflow serving。

最后4600W文书算一下全部跑完要10多天。DGCNN还需要测一测，比BiLSTM又降了一到两个点。由于数据本身就很少，用来作为dev_set的数据就更少了，对测试准确性很有问题，比如有的字段dev_set一共才20个测试数据，一个数据5%，这误差谁顶得住啊。DGCNN理论上还能再快一些，BiLSTM不能并行搞，CNN可以，我感觉这个NER的问题局部特征就够了，理论上CNN可以和BiLSTM差不多。这样可以优化到7天左右。肯定还有优化的余地，不过我是有些累了，为啥不多搞一张卡？

## 3. 踩坑

### 1. 工程

第一次踩坑是去行内使用gpu镜像的时候。
行内给我提供了一个没有GPU的服务器，我第一次去部署的是同事写的模型，为了在只有CPU的机器上测试，除了和tensorflow-gpu，我还在镜像中同时安装了tensorflow。

后来由于行内的容器应用部署平台不是特别好操作（那边的业务老师都配置的不熟练，如果一定要配置我觉得我应该能指导她配置好），就先提供了一台有GPU的服务器。然后在镜像跑起来以后，用tf的api看不到GPU。先用[命令](https://arnon.dk/check-cuda-installed/)nvidia-smi和nvcc看一下，驱动和CUDA有没有问题，然后一开始的机器容器里面nvidia-smi没有输出的，后来换了台机器就有输出了，依然看不到GPU，确认基础镜像没有问题，把tensorflow删掉就看到了GPU。咨询了小磊，还有jesse，他们给了我不少指导。是不是需要mount device，我还得自己也搞一遍才能确认，实际看起来似乎是不用的。

### 2. notebook

在搞模型前我先用notebook做一下POC。然后NOTEBOOK有个小坑，我把optimizer和model放在了不同的codeblock里面，然后我每次改模型都只重新eval了model所在的codeblock，结果怎么训练都训练不动。其实是应该把optimizer也重新eval一下，这样backward的时候才能改变新模型的参数。

### 3. pytorch

有个小坑也踩了，就是我把几个cnn_layer放在了一个list里面，然后作为成员变量。这样是不对的，可以打印所有的参数，发现其中并没有这些layer，后来就发现了这个问题。

### 4. torchtext
torchtext提供了非常方便的API，自动pad，自动管理vocabulary，切分train_set和dev_set。
注意切分的时候，是随机切分的，所以如果restore from checkpoint以后接着训练，如果随机状态不确定，就会导致dev_set的数据不断流入train_set。这个问题其实很好发现，因为多次如此操作以后，正确率会接近100。想想这也可以变相用来测试模型是否有足够的表达能力。

还有就是模型有问题的时候可以输出错误的例子，从中找寻bug所在。后来一个bug是因为torchtext的buildvocab方法用的不对，点进去看了源代码，然后从一个tokenlist去生成vocab。

我BiLSTM和DGCNN一开始没有用pretrain的embedding，结果在新的数据上，表现很差。所以后来用了一份embedding，在随手抓的数据上也好了不少，但是比起bert还是有肉眼可见的差距。

先记这么多吧，也是新司机上路，还是有很多东西要学要整，慢慢记。
